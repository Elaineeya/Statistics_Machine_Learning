---
title: "Bayesian Lab3"
author: "Xuan Wang & Lepeng Zhang"
date: "2024-05-13"
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1  Gibbs sampling for the logistic regression

## 1a)

```{r,  echo = TRUE, warning=FALSE, message=FALSE}
library("mvtnorm")
library("BayesLogit")
set.seed(12345)
raw_data <- read.table("WomenAtWork.dat", header= TRUE)
y <- raw_data$Work
X <- as.matrix(raw_data[,2:ncol(raw_data)])
Xnames <- colnames(X)

n <- dim(X)[1]
Npar <- dim(X)[2]
tau <- 3
b <- as.matrix(rep(0,Npar))
B <- (tau^2)*diag(Npar)
k <-  y-0.5
init_beta <- rmvnorm(1, mean = b, sigma = B)
n_iter <- 1000
w <- rep(0,n)
post_beta <-matrix(0, nrow= n_iter, ncol= Npar)

for (i in 1:n_iter){
  for (j in 1:n){
    a <- X[j,]%*%t(init_beta)
    w[j] <- rpg(1, abs(a))
  }
  O <- diag(w)
  V <- solve(t(X)%*%O%*%X + solve(B))
  m <- V%*%(t(X)%*%k + solve(B)%*%b)
  beta <- rmvnorm(1, m, V)
  init_beta <- beta
  post_beta[i,] <- beta
}

par(mfrow= c(2, 2))
for (i in 1:Npar){
 plot(post_beta[,i], main= paste("Trace of",Xnames[i]), type= "l", xlab='Iterations', ylab=Xnames[i])
}
```


```{r}
IFs <- apply(post_beta, 2, function(x){
  acf_x <- acf(x, plot = FALSE)$acf[,,1]
  return(1 + 2 * sum(acf_x[-1]))
})
names(IFs) <- Xnames
IFs
```

In the terms of IFs and trace plots, the sampling has not converged well except for Constant and NBigChild as their IFs are less than 1.1. 

## 1b)

```{r,  echo = TRUE, warning=FALSE, message=FALSE}
new_x <- c(1, 22, 12, 7, 38, 1, 0)
logistic <- function(x){
  1/(1+exp(-x))
}
prob <- logistic(new_x %*% t(post_beta))
quantile(prob, c(0.05, 0.95))
#hist(prob, freq=FALSE, breaks = 500)
```


## 2a)

```{r,  echo = TRUE, warning=FALSE, message=FALSE}
set.seed(12345)
raw_data <- read.table("eBayNumberOfBidderData_2024.dat", header = TRUE)
y <- raw_data[,1]
X <- as.matrix(raw_data[,-1])
Xnames <- colnames(X)
m1 <- glm(y~X[,-1], raw_data, family='poisson')
summary(m1)
```
VerifyID, Sealed, MinBidShare are important.   

## 2b)  

```{r,  echo = TRUE, warning=FALSE, message=FALSE}
n <- dim(X)[1]
p <- dim(X)[2]
mu <- rep(0, p)
Sigma <- 100*solve(t(X)%*%X)

log_posterior <- function(beta,X,y){
  log_likelihood = sum(y*(X %*% beta)) - sum(exp(X %*% beta))
  log_prior = dmvnorm(beta, mu, Sigma, log = TRUE)
  return(log_likelihood + log_prior)
}

optim_result <- optim(coef(m1), log_posterior, gr=NULL, X, y,  method=c('BFGS'), control=list(fnscale=-1), hessian= TRUE)

beta_hat <- optim_result$par
names(beta_hat) <- Xnames
print('The posterior modes:')
print(beta_hat)

J <- -optim_result$hessian
inv_J <- solve(J)
rownames(inv_J) <- Xnames
colnames(inv_J) <- Xnames
print('The posterior covariance:')
print(inv_J)
```
The posterior mode is close to the coefficients in 1a).  

## 2c)

```{r,  echo = TRUE, warning=FALSE, message=FALSE}
logPostFunc_poi <- function(theta, X, y){
  log_likelihood = sum(y*(X %*% theta)) - sum(exp(X %*% theta))
  log_prior = dmvnorm(theta, mu, Sigma, log = TRUE)
  return(log_likelihood + log_prior)
}

RWMSampler <- function(Func, c_value, init_theta, X, y, n_samples=10000){
  return_list <- list()
  draws <- matrix(0, n_samples, ncol(X))
  i <- 1
  draws[i,] <- init_theta
  count_accept <- 0
  
  while(i<n_samples){
    i = i + 1
    theta_p <- as.numeric(rmvnorm(1,  draws[i-1,], c_value*inv_J))
    acc_pro <- min(1, exp(Func(theta_p, X, y)-Func(draws[i-1,], X, y)))
    
    if(runif(1)<acc_pro){
       draws[i,] <- theta_p
       count_accept <- count_accept+1
    } else{
      draws[i,] <- draws[i-1,]
    }
  }
  accept_rate <- count_accept/(n_samples-1)
  return_list$draws <- draws
  return_list$accept_rate <- accept_rate
  return(return_list)
}

set.seed(12345)
theta_0 <- rep(0, ncol(X))
result <- RWMSampler(logPostFunc_poi, c_value = 0.5, init_theta = theta_0, X = X, y = y)
draws <- result$draws
accept_rate <- result$accept_rate

cat(paste0('The acceptance probability is ',round(accept_rate*100,1),'%.'))

par(mfrow = c(2, 2))
for (i in 1:p) {
  hist(draws[,i], main = paste("Hist of", Xnames[i]), breaks = 100, freq = FALSE)
}
for (i in 1:p) {
  plot(draws[ ,i], main = paste("Trace of", Xnames[i]), type = "l", xlab='Iterations', ylab= Xnames[i])
}
```

From both histograms and trace plots, we can see that they all converged.  


## 2d)

```{r,  echo = TRUE, warning=FALSE, message=FALSE}
X_new <- c(1, 1, 0, 1, 0, 1, 0, 1.2, 0.8)

num_bidder <- c()
for (i in 1:nrow(draws)){
  num_bidder[i] <- rpois(1, lambda = exp(draws[i,] %*% X_new))
}

hist(num_bidder,  breaks = seq(-0.5, 8.5, 1), freq = FALSE)

cat(paste0('The prob that no bidders is ',round(mean(num_bidder==0),2),'.'))
```



# Question 3 Time series models in Stan

## 3a)

```{r, 3a, echo = TRUE, error=FALSE, message=FALSE, warning=FALSE}
# Load required library
library(rstan)
set.seed(12345)
# Function to simulate AR(1) process
simulate_ar1 <- function(mu, phi, sigma_sq, T) {
  x <- numeric(T)
  x[1] <- mu
  for (t in 2:T) {
    e_t <- rnorm(1, mean = 0, sd = sqrt(sigma_sq))
    x[t] <- mu + phi * (x[t-1] - mu) + e_t
  }
  return(x)
}

# Simulate AR(1) process
mu <- 9
sigma_sq <- 4
T <- 250
phi_values <- seq(-1, 1, by = 0.2)
par(mfrow = c(2, 2)) 
for (phi in phi_values) {
  x <- simulate_ar1(mu, phi, sigma_sq, T)
  plot(x, main = paste("AR(1) process with phi =", phi), type = "l")
}
par(mfrow = c(1, 1))  # Reset the plot layout


```

$\phi$ close to -1: The plot shows a strong negative autocorrelation. That is, if the process was high (or low) at one point, it is likely to be low (or high) at the next point. This aligns with the first plot that there is a wider up and down range as the process frequently changes between high and low values.

$\phi$ close to 0: The plot shows there are not too much autocorrelation. The values in the process are not influenced by their preceding values. As a result, it can be seen that the fluctuation becomes smooth and within a regular range, meaning the process appears more random and less predictable.

$\phi$ close to 1: The plot shows a strong positive autocorrelation. If the process was high (or low) at one point, it is likely to be high (or low) at the next point. This can lead to irregular up and down fluctuations as the process tends to maintain its current state for longer periods, as the last plot shown.

```{r, 3b, echo = TRUE, warning=FALSE, message=FALSE, error=FALSE, results='hide'}
set.seed(12345)
# Stan code
stan_code <- "
data {
  int<lower=0> T;
  vector[T] y;
}
parameters {
  real mu;
  real<lower=-1, upper=1> phi;
  real<lower=0> sigma;
}
model {
  vector[T] nu;
  nu[1] <- mu;
  for (t in 2:T) {
    nu[t] <- mu + phi * (y[t-1] - mu);
  }
  y ~ normal(nu, sigma);
}
"

# Compile Stan model
stan_model <- stan_model(model_code = stan_code)

# Simulate two AR(1) processes
x1 <- simulate_ar1(mu, 0.3, sigma_sq, T)
y1 <- simulate_ar1(mu, 0.97, sigma_sq, T)

# Fit Stan model to the simulated data
fit_x1 <- sampling(stan_model, data = list(T = T, y = x1))
fit_y1 <- sampling(stan_model, data = list(T = T, y = y1))


```

```{r, 3b1, echo = TRUE}
# Print the results
print(summary(fit_x1)$summary)
print(summary(fit_y1)$summary)

```

From above results, for the first dataset from $\phi$=0.3,

| parameters | mean  | 95% credible intervals | the number of effective samples |
|--------------|--------------|------------------|--------------------------|
| mu         | 9.191 | 8.895-9.489            | 3281                            |
| phi        | 0.242 | 0.201-0.369            | 3660                            |
| sigma      | 1.849 | 1.689-2.022            | 4044                            |

for the second dataset from $\phi$=0.97,

| parameters | mean  | 95% credible intervals | the number of effective samples |
|--------------|--------------|------------------|--------------------------|
| mu         | 8.075 | 4.598-11.991           | 2518                            |
| phi        | 0.969 | 0.935-0.996            | 1881                            |
| sigma      | 1.914 | 1.749-2.093            | 2538                            |

```{r, 3b21, echo = TRUE}

stan_trace(fit_x1, pars=c("mu"))
stan_trace(fit_x1, pars=c("phi"))
stan_trace(fit_x1, pars=c("sigma"))
```

Above three trace plots are for the first dataset from $\phi$=0.3, it can be seen that all three parameters converge quite well in the stationery distribution, the plots are also symmetric around the mean value.

```{r, 3b22, echo = TRUE}

stan_trace(fit_y1, pars=c("mu"))
stan_trace(fit_y1, pars=c("phi"))
stan_trace(fit_y1, pars=c("sigma"))
```

Above three trace plots are for the second dataset from $\phi$=0.97, it can be seen that the parameter mu doesn't converge well, the variance of the points is bigger. This is because high $\phi$ leads to a strong positive autocorrelation. 

```{r, 3b23, echo = TRUE, warning=FALSE}

pairs(fit_x1, pars = c("mu", "phi"), main = "The first dataset from phi=0.3")
pairs(fit_y1, pars = c("mu", "phi"), main = "The second dataset from phi=0.97")

```

From above joint posterior plots of $\mu$ and $\phi$, the first dataset generates symmetric ellipsoid shape, which could be possibly a multivariate normal distribution. While the second dataset generates a deformed ellipsoid, which are hard to guess the distribution.





